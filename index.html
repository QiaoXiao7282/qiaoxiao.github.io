<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title> </title>
        <link rel="stylesheet" href="./css/default.css" />
        <link rel="stylesheet" href="./css/syntax.css" />
        <link rel="shortcut icon" type="image/x-icon" href="./images/favicon.ico" />
        <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://chaoxuprime.com/mathjax_conf.js">

        </script>

    </head>

    <body>

        <article role="main">
            <h1 id="article-title"> </h1>
    <br />

<h1>Biography</h1>
<table class="postindex">
<ul>
    <tr>
        <td> 
            Pingchuan Ma is a Postdoctoral Researcher at Imperial College London/Meta AI. He received the PhD degree in Computer Science and MSc in Machine Learning from Imperial College London, supervised by <a href="https://ibug.doc.ic.ac.uk/people/mpantic">Prof. Maja Pantic</a> and <a href="https://ibug.doc.ic.ac.uk/people/spetridis">Dr. Stavros Petridis</a>.
            <p>[<a href="https://scholar.google.com/citations?user=ZUW256sAAAAJ&hl=zh-CN&oi=sra">Google Scholar</a>] [<a href="https://github.com/mpc001">Github</a>]</p>
        </td>
    </tr>
</ul>
</table> 


<h1>Publications</h1>
<table class="postindex">
<ul>

    <tr>
        <td> <b>SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision</b>
             <a href="https://arxiv.org/abs/2303.17200"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a>
             <p>Xubo Liu, Egor Lakomkin, Konstantinos Vougioukas, Pingchuan Ma, Honglie Chen, Ruiming Xie, Morrie Doulaty, Niko Moritz, Jáchym Kolář, Stavros Petridis, Maja Pantic, Christian Fuegen</p>

         </td>
        <td class="right">CVPR, 2023</td>
    </tr>

    <tr>
        <td> <b>Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels</b>
             <a href="https://arxiv.org/abs/2303.14307"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a>
             <a href="https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages"><i class="material-icons" style="font-size: 1.1em;">code</i></a>
             <p>Pingchuan Ma, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, Maja Pantic</p>

         </td>
        <td class="right">ICASSP, 2023</td>
    </tr>

    <tr>
        <td> <b>Learning Cross-lingual Visual Speech Representations</b>
             <a href="https://arxiv.org/abs/2303.09455"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <p>Andreas Zinonos, Alexandros Haliassos, Pingchuan Ma, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2023</td>
    </tr>

    <tr>
        <td> <b>Jointly Learning Visual and Auditory Speech Representations from Raw Data</b>
             <a href="https://arxiv.org/abs/2212.06246"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <p>Alexandros Haliassos, Pingchuan Ma, Rodrigo Mira, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">ICLR, 2023</td>
    </tr>

    <tr>
        <td> <b>Visual Speech Recognition for Multiple Languages in the Wild</b>
             <a href="https://arxiv.org/abs/2202.13084"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a>
             <a href="https://mpc001.github.io/lipreader.html"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <a href="https://github.com/mpc001/Visual_Speech_Recognition_for_Multiple_Languages"><i class="material-icons" style="font-size: 1.1em;">code</i></a>
             <p>Pingchuan Ma, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">Nature Machine Intelligence, 2022</td>
    </tr> 

    <tr>
        <td> <b>Training Strategies for Improved Lip-Reading</b>
             <a href="https://arxiv.org/abs/2209.01383"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a>
             <a href="https://sites.google.com/view/audiovisual-speech-recognition"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks"><i class="material-icons" style="font-size: 1.1em;">code</i></a>
             <p>Pingchuan Ma*, Yujiang Wang*, Stavros Petridis, Jie Shen, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2022</td>
    </tr> 

    <tr>
        <td> <b>LiRA: Learning Visual Speech Representations from Audio through Self-supervision</b>
             <a href="https://arxiv.org/abs/2106.09171"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a>
             <p>Pingchuan Ma*, Rodrigo Mira*, Stavros Petridis, Björn W. Schuller, Maja Pantic</p>
         </td>
        <td class="right">Interspeech, 2021</td>
    </tr>    

    <tr>
        <td> <b>End-to-End Audio-visual Speech Recognition with Conformers</b>
             <a href="https://arxiv.org/abs/2102.06657"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <p>Pingchuan Ma, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2021</td>
    </tr>

    <tr>
        <td> <b>Detecting Adversarial Attacks on Audio-visual Speech Recognition</b> 
            <a href="https://arxiv.org/abs/1912.08639"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
            <a href="av_adversarial_examples.html"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a>
            <p>Pingchuan Ma, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2021</td>
    </tr>

    <tr>
        <td> <b>Towards Practical Lipreading with Distilled and Efficient Models</b> 
             <a href="https://arxiv.org/abs/2007.06504"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <a href="https://sites.google.com/view/audiovisual-speech-recognition"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks"><i class="material-icons" style="font-size: 1.1em;">code</i></a>
             <p>Pingchuan Ma*, Brais Martinez*, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2021</td>
    </tr>

    <tr>
        <td> <b>Towards Pose-invariant Lip-Reading</b> 
             <a href="https://arxiv.org/abs/1911.06095"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <p>Shiyang Cheng*, Pingchuan Ma*, Georgios Tzimiropoulos, Stavros Petridis, Adrian Bulat, Jie Shen, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2020</td>
    </tr>

    <tr>
        <td> <b>Lipreading Using Temporal Convolutional Networks</b> 
             <a href="https://arxiv.org/abs/2001.08702"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <a href="https://sites.google.com/view/audiovisual-speech-recognition"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks"><i class="material-icons" style="font-size: 1.1em;">code</i></a>
             <p>Brais Martinez, Pingchuan Ma, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2020</td>
    </tr>

    <tr>
        <td> <b>Investigating the Lombard Effect Influence on End-to-End Audio-visual Speech Recognition</b> 
             <a href="https://arxiv.org/abs/1906.02112"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <a href="./images/interspeech2019.jpg"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <p>Pingchuan Ma, Stavros Petridis, Maja Pantic</p>
         </td>
        <td class="right">Interspeech, 2019</td>
    </tr>

    <tr>
        <td> <b>Audio-visual Speech Recognition with a Hybrid CTC/Attention Architecture</b> 
             <a href="https://arxiv.org/abs/1810.00108"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <a href="images/slt2018.jpg"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <p>Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Georgios Tzimiropoulos, Maja Pantic</p>
         </td>
        <td class="right">SLT, 2018</td>
    </tr>


    <tr>
        <td> <b>End-to-end Audio-visual Speech Recognition</b> 
             <a href="https://arxiv.org/abs/1802.06424"><i class="material-icons" style="font-size: 1.1em;";>content_copy</i></a> 
             <a href="https://sites.google.com/view/audiovisual-speech-recognition"><i class="material-icons" style="font-size: 1.1em;">slideshow</i></a> 
             <a href="https://github.com/mpc001/end-to-end-lipreading"><i class="material-icons" style="font-size: 1.1em;">code</i></a>
             <p>Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Feipeng Cai, Georgios Tzimiropoulos, Maja Pantic</p>
         </td>
        <td class="right">ICASSP, 2018</td>
    </tr>


</ul>
</table> 


</content_copy>


</body>
</html>
